#I. DEFINITION
##PROJECT OVERVIEW
As post-secondary education becomes a common experience of the commonwealth, the budget of an education institution is often limited and funding is competitive. While working closely with many colleges and students, knowing the expectation of their investment is crucial for both. The project aims to figure out whether the allocation of expenditure can help predicting the performance of institution - which is defined by the number of awards, degree, and certificate granted - as it has been widely used to evaluate the performance of higher education institutes.
http://www.ncsl.org/research/education/performance-funding.aspx 
http://www.ed.gov/college-completion
This project is inspired by Udacity's capstone project guidance and uses dataset downloaded from Integrated Postsecondary Education Data System Delta Cost Project Database, which include the data from the academy year 1987-1988 to 2012-2013. The Delta Cost Project Database is derived from the Integrated Postsecondary Education Data System (IPEDS) surveys on finance, enrollment, staffing, completions, and student aid, and the data have been translated into analytical formats to allow for longitudinal analyses of trends in postsecondary education with a focus on revenues and expenditure.

##PROBLEM STATEMENT
The question asked with this project is whether expenditure can be used to predict completion number and if expenditure pattern can make a better predictor for the same target. Expenditure categories will be retrieved from the data set acquired through delta cost project database, The goal of this project is to identify if the expenditure pattern will be a better reference to predict high completion number than original data. 
Dealing with this problem, the target variable and the expenditure variables will be extracted from data set. 75% of randomly selected data set will be used as the training set, while the rest will be the testing set. The pattern of expenditure has to be found through expenditure variables in the training set. Performance of the model trained by original data will be compared with the performance of the model trained by expenditure pattern.

##METRICS
The models being evaluated will be regression models, and the estimate of error between predicted values and actual value would be the reference of performance. The options for regression models include Mean Absolute Error (MAE), Root of Mean Squared Error (RMSE), and R-squared score (R2).
Mean Absolute Error calculates the average of difference between predicted value and actual value. 
Root Mean Squared Error takes the root of the average squared distance between predicted value and actual value. Compare with MAE, RMSE weighs more on the error that is further away from mean. Both MAE and RMSE are negative oriented â€“ meaning the less the error the higher the accuracy is. When MAE equals to RMSE, it means every error are of the same magnitude.
R-squared calculates the variance of the true data set, and calculates the proportion that the predicted data can be accountable for. The residual of the variance indicates the variance caused by the difference between actual data and predicted data.  The much variance being explained by predicted data the higher the score is, which also indicates higher accuracy. R2, however, inflates when adding more predictors (variables) to the metric. The variance caused by predicted value hence increase even without model improvement. Adjusted R-squared score (Adj R2) is developed to counter the inflation and adding penalty for extra variables entering the metric. Adj R2 is always smaller or equals to R2 score. Predicted R2 is another score that helps to determine if a model fits the original data but less capable of providing valid predictions on new data points.
Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) are also popular tools to evaluate the information loss of a model. The computation of AIC and BIC is complicated, but the concept is based on maximum likelihood estimate of the model parameters, which is the estimate of parameters that gives the highest probability. Log of the value of maximum likelihood function is taken, which ranges from negative infinity to 0. Negative 2 will multiply the value of log. For AIC, this will add 2 times of the total number of parameters. For BIC, the number of parameter will multiply the log of the number of observations, which is the number of data points in our case. AIC and BIC are all numbers for comparison and definite value cannot be interpreted. For both criteria, the model with smaller number of result will be preferred. AIC penalize additional parameter less heavily than BIC, and BIC should be always smaller than AIC when evaluating the same model. 
When comparing MAE and RMSE, RMSE puts more weights on the larger error, and MAE behaves less sensitive to outliers. When comparing the model of original data and the model of transformed data set, it is likely the dimensionality changes. To avoid the inflation affects the R2 score should not be used as the metric of evaluation but Adj R2 can be used instead. While R2 ranges from 0 to 1, Adj R2 can be negative, which makes it more complicated than R2 to be understood. This feature makes Adj R2 rather easy to understand and straight forward. AIC and BIC are commonly used tools while evaluating model performance, and both have the feature that penalize over-fitting. This makes AIC and BIC the most stands out metrics among above. AIC and BIC provide relative reference of model performance, but no absolute reference that determine whether the model predicts accurately or not. To add further reference, Adj R2 will be used.
